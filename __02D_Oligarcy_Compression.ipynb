{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOGxnv/YIMUjK6JHzQ4rlin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanfuse123/Airship.jl/blob/master/__02D_Oligarcy_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAU6kMt2FF1x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile encoder.py\n",
        "# --- DVD Hierarchical Encoder ---\n",
        "# This script downloads a DVD ISO and applies the hierarchical compression technique\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import hashlib\n",
        "import hmac\n",
        "import struct\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "import time\n",
        "import json\n",
        "import argparse\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Block size constants - using 64KB as our optimal block size\n",
        "CHUNK_SIZE = 64 * 1024  # 64KB chunks\n",
        "NUM_STAGES = 37         # Number of transformation stages\n",
        "NUM_WORKERS = os.cpu_count() or 4  # Use all available cores\n",
        "\n",
        "class HierarchicalCompressor:\n",
        "    \"\"\"\n",
        "    Class to manage the hierarchical compression process for DVD data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, master_seed=None, num_stages=NUM_STAGES,\n",
        "                 chunk_size=CHUNK_SIZE, num_workers=NUM_WORKERS):\n",
        "        # Initialize with random seed if none provided\n",
        "        if master_seed is None:\n",
        "            master_seed = os.urandom(32)\n",
        "        elif isinstance(master_seed, str):\n",
        "            # Hash the string to get a fixed-length seed\n",
        "            master_seed = hashlib.sha256(master_seed.encode()).digest()\n",
        "\n",
        "        self.master_seed = master_seed\n",
        "        self.num_stages = num_stages\n",
        "        self.chunk_size = chunk_size\n",
        "        self.num_workers = num_workers\n",
        "        self.debug_mode = False\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"Log debug messages if debug mode is enabled.\"\"\"\n",
        "        if self.debug_mode:\n",
        "            print(f\"DEBUG: {message}\")\n",
        "\n",
        "    # --- Core PRNG and Transformation Functions ---\n",
        "\n",
        "    def derive_stage_seed(self, chunk_index, stage_index):\n",
        "        \"\"\"Derive a stage-specific seed.\"\"\"\n",
        "        data = f\"{chunk_index}_{stage_index}\".encode()\n",
        "        h = hmac.new(self.master_seed, data, hashlib.sha256)\n",
        "        return h.digest()\n",
        "\n",
        "    def custom_prng_decision(self, stage_seed):\n",
        "        \"\"\"Generate a binary decision (0 or 1) from the stage seed.\"\"\"\n",
        "        hash_val = hashlib.sha256(stage_seed).digest()\n",
        "        decision = hash_val[0] & 1\n",
        "        return decision\n",
        "\n",
        "    def generate_mask_block(self, stage_seed, start_counter, block_size):\n",
        "        \"\"\"Generate a portion of the mask for XOR transformation.\"\"\"\n",
        "        digest_size = 32  # SHA-256 produces 32 bytes\n",
        "        num_digests = (block_size + digest_size - 1) // digest_size\n",
        "\n",
        "        # Pre-allocate buffer\n",
        "        mask = bytearray(num_digests * digest_size)\n",
        "        offset = 0\n",
        "\n",
        "        # Generate the mask block\n",
        "        for i in range(num_digests):\n",
        "            counter = start_counter + i\n",
        "            digest = hmac.new(stage_seed, struct.pack('>I', counter),\n",
        "                             hashlib.sha256).digest()\n",
        "            mask[offset:offset + len(digest)] = digest\n",
        "            offset += len(digest)\n",
        "\n",
        "        # Convert to NumPy array and trim\n",
        "        return np.frombuffer(mask[:block_size], dtype=np.uint8)\n",
        "\n",
        "    def apply_xor_transformation(self, data, mask):\n",
        "        \"\"\"Apply XOR transformation to data using the given mask.\"\"\"\n",
        "        return np.bitwise_xor(data, mask)\n",
        "\n",
        "    # --- Chunk Processing ---\n",
        "\n",
        "    def process_chunk(self, chunk_index, original_chunk):\n",
        "        \"\"\"Process a single chunk through all stages.\"\"\"\n",
        "        try:\n",
        "            # Convert input to NumPy array for efficient processing\n",
        "            chunk_data = np.frombuffer(original_chunk, dtype=np.uint8).copy()\n",
        "            recorded_decisions = []\n",
        "\n",
        "            # Process each stage sequentially\n",
        "            for stage in range(self.num_stages):\n",
        "                # Generate stage seed\n",
        "                stage_seed = self.derive_stage_seed(chunk_index, stage)\n",
        "\n",
        "                # Make binary decision\n",
        "                decision = self.custom_prng_decision(stage_seed)\n",
        "                recorded_decisions.append(decision)\n",
        "\n",
        "                # Skip transformation if decision is 0\n",
        "                if decision == 0:\n",
        "                    continue\n",
        "\n",
        "                # Process the chunk in blocks to save memory\n",
        "                for offset in range(0, len(chunk_data), self.chunk_size):\n",
        "                    # Adjust block size for the last block\n",
        "                    current_block_size = min(self.chunk_size, len(chunk_data) - offset)\n",
        "\n",
        "                    # Generate mask block\n",
        "                    counter_start = offset // 32  # Each SHA-256 digest is 32 bytes\n",
        "                    mask_block = self.generate_mask_block(stage_seed, counter_start,\n",
        "                                                        current_block_size)\n",
        "\n",
        "                    # Apply transformation to this block\n",
        "                    chunk_data[offset:offset + current_block_size] = self.apply_xor_transformation(\n",
        "                        chunk_data[offset:offset + current_block_size],\n",
        "                        mask_block\n",
        "                    )\n",
        "\n",
        "            # Convert back to bytes\n",
        "            return chunk_data.tobytes(), recorded_decisions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_chunk: {str(e)}\")\n",
        "            # Return original data and empty decisions on error\n",
        "            return original_chunk, []\n",
        "\n",
        "    # --- Parallel Processing ---\n",
        "\n",
        "    def process_multiple_chunks(self, chunks):\n",
        "        \"\"\"Process multiple chunks in parallel.\"\"\"\n",
        "        transformed_chunks = [None] * len(chunks)\n",
        "        all_decisions = [None] * len(chunks)\n",
        "\n",
        "        # Define the worker function\n",
        "        def process_chunk_worker(args):\n",
        "            chunk_idx, chunk_data = args\n",
        "            transformed, decisions = self.process_chunk(chunk_idx, chunk_data)\n",
        "            return chunk_idx, transformed, decisions\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
        "            # Submit tasks\n",
        "            futures = [executor.submit(process_chunk_worker, (i, chunk))\n",
        "                      for i, chunk in enumerate(chunks)]\n",
        "\n",
        "            total = len(futures)\n",
        "            completed = 0\n",
        "            last_percentage = 0\n",
        "\n",
        "            # Collect results\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    chunk_idx, transformed, decisions = future.result()\n",
        "                    transformed_chunks[chunk_idx] = transformed\n",
        "                    all_decisions[chunk_idx] = decisions\n",
        "\n",
        "                    # Update completion percentage (only at 10% intervals)\n",
        "                    completed += 1\n",
        "                    percentage = int((completed / total) * 100)\n",
        "                    if percentage >= last_percentage + 10:\n",
        "                        print(f\"Processing: {percentage}% complete ({completed}/{total} chunks)\")\n",
        "                        last_percentage = percentage\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in worker thread: {str(e)}\")\n",
        "\n",
        "        # Check for unprocessed chunks\n",
        "        for i in range(len(chunks)):\n",
        "            if transformed_chunks[i] is None:\n",
        "                print(f\"Warning: Chunk {i} was not processed, using original\")\n",
        "                transformed_chunks[i] = chunks[i]\n",
        "                all_decisions[i] = []\n",
        "\n",
        "        return transformed_chunks, all_decisions\n",
        "\n",
        "    # --- DVD Processing ---\n",
        "\n",
        "    def split_dvd_into_chunks(self, dvd_data):\n",
        "        \"\"\"Split a DVD into fixed-size chunks.\"\"\"\n",
        "        chunks = []\n",
        "        for i in range(0, len(dvd_data), self.chunk_size):\n",
        "            chunk = dvd_data[i:i + self.chunk_size]\n",
        "            chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    def process_dvd(self, dvd_data):\n",
        "        \"\"\"Process an entire DVD.\"\"\"\n",
        "        original_size = len(dvd_data)\n",
        "        print(f\"Processing DVD of size {original_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = self.split_dvd_into_chunks(dvd_data)\n",
        "        print(f\"Split into {len(chunks)} chunks of {self.chunk_size / 1024:.1f} KB each\")\n",
        "\n",
        "        # Process all chunks in parallel\n",
        "        start_time = time.time()\n",
        "        transformed_chunks, all_decisions = self.process_multiple_chunks(chunks)\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"Processing completed in {process_time:.2f} seconds\")\n",
        "\n",
        "        return transformed_chunks, all_decisions, original_size\n",
        "\n",
        "    # --- Metadata Handling ---\n",
        "\n",
        "    def save_reconstruction_metadata(self, all_decisions, original_size, output_file):\n",
        "        \"\"\"Save the reconstruction metadata to a file.\"\"\"\n",
        "        metadata = {\n",
        "            'master_seed': self.master_seed.hex(),\n",
        "            'num_stages': self.num_stages,\n",
        "            'chunk_size': self.chunk_size,\n",
        "            'original_size': original_size,\n",
        "            'num_chunks': len(all_decisions),\n",
        "            'decisions': all_decisions\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "\n",
        "        print(f\"Saved reconstruction metadata to {output_file}\")\n",
        "\n",
        "        # Calculate and print metadata size\n",
        "        metadata_size = os.path.getsize(output_file)\n",
        "        compression_ratio = original_size / metadata_size\n",
        "        print(f\"Metadata size: {metadata_size / 1024:.2f} KB\")\n",
        "        print(f\"Compression ratio: {compression_ratio:.1f}:1\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"\n",
        "    Download a file from URL with progress updates at 10% intervals.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading {url} to {output_path}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    # Make request\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Get file size\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    if total_size == 0:\n",
        "        print(\"Warning: Content length not available, download progress cannot be tracked\")\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    else:\n",
        "        # Download with progress updates\n",
        "        bytes_downloaded = 0\n",
        "        last_percentage = 0\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    bytes_downloaded += len(chunk)\n",
        "\n",
        "                    # Update progress at 10% intervals\n",
        "                    percentage = int((bytes_downloaded / total_size) * 100)\n",
        "                    if percentage >= last_percentage + 10:\n",
        "                        print(f\"Download: {percentage}% complete ({bytes_downloaded/1024/1024:.1f}MB/{total_size/1024/1024:.1f}MB)\")\n",
        "                        last_percentage = percentage\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Download completed in {elapsed:.1f} seconds\")\n",
        "\n",
        "def get_alpine_iso_url():\n",
        "    \"\"\"Return URL for Alpine Linux ISO.\"\"\"\n",
        "    return \"https://dl-cdn.alpinelinux.org/alpine/v3.18/releases/x86_64/alpine-standard-3.18.0-x86_64.iso\"\n",
        "\n",
        "def get_ubuntu_iso_url():\n",
        "    \"\"\"Return URL for Ubuntu ISO.\"\"\"\n",
        "    return \"https://releases.ubuntu.com/22.04.3/ubuntu-22.04.3-desktop-amd64.iso\"\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    print(f\"Calculating MD5 of {file_path}...\")\n",
        "    md5_hash = hashlib.md5()\n",
        "\n",
        "    # Get file size\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    bytes_processed = 0\n",
        "    last_percentage = 0\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read in 64kb chunks\n",
        "        for chunk in iter(lambda: f.read(64 * 1024), b\"\"):\n",
        "            md5_hash.update(chunk)\n",
        "\n",
        "            # Update progress at 10% intervals\n",
        "            bytes_processed += len(chunk)\n",
        "            percentage = int((bytes_processed / file_size) * 100)\n",
        "            if percentage >= last_percentage + 10:\n",
        "                print(f\"MD5 calculation: {percentage}% complete\")\n",
        "                last_percentage = percentage\n",
        "\n",
        "    result = md5_hash.hexdigest()\n",
        "    print(f\"MD5: {result}\")\n",
        "    return result\n",
        "\n",
        "# --- Main Function ---\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"DVD Hierarchical Encoder\")\n",
        "\n",
        "    source_group = parser.add_mutually_exclusive_group(required=True)\n",
        "    source_group.add_argument(\"--url\", help=\"URL to download DVD ISO\")\n",
        "    source_group.add_argument(\"--alpine\", action=\"store_true\", help=\"Download Alpine Linux ISO\")\n",
        "    source_group.add_argument(\"--ubuntu\", action=\"store_true\", help=\"Download Ubuntu ISO\")\n",
        "    source_group.add_argument(\"--file\", help=\"Path to local DVD ISO file\")\n",
        "    source_group.add_argument(\"--image\", help=\"Path to image file in models/ directory\")\n",
        "\n",
        "    parser.add_argument(\"--output-dir\", default=\"output\", help=\"Output directory\")\n",
        "    parser.add_argument(\"--seed\", default=None, help=\"Master seed (string)\")\n",
        "    parser.add_argument(\"--chunk-size\", type=int, default=CHUNK_SIZE,\n",
        "                        help=f\"Chunk size in bytes (default: {CHUNK_SIZE})\")\n",
        "    parser.add_argument(\"--stages\", type=int, default=NUM_STAGES,\n",
        "                        help=f\"Number of transformation stages (default: {NUM_STAGES})\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    # Get DVD ISO file\n",
        "    iso_path = None\n",
        "\n",
        "    if args.url:\n",
        "        iso_path = os.path.join(args.output_dir, \"downloaded.iso\")\n",
        "        download_file(args.url, iso_path)\n",
        "\n",
        "    elif args.alpine:\n",
        "        iso_path = os.path.join(args.output_dir, \"alpine.iso\")\n",
        "        download_file(get_alpine_iso_url(), iso_path)\n",
        "\n",
        "    elif args.ubuntu:\n",
        "        iso_path = os.path.join(args.output_dir, \"ubuntu.iso\")\n",
        "        download_file(get_ubuntu_iso_url(), iso_path)\n",
        "\n",
        "    elif args.file:\n",
        "        iso_path = args.file\n",
        "        print(f\"Using local file: {iso_path}\")\n",
        "\n",
        "    elif args.image:\n",
        "        # Ensure models directory exists\n",
        "        models_dir = \"models\"\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Get full path within models directory\n",
        "        iso_path = os.path.join(models_dir, args.image)\n",
        "\n",
        "        if not os.path.exists(iso_path):\n",
        "            print(f\"Error: Image file {iso_path} does not exist\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    # Calculate MD5 of original\n",
        "    original_md5 = calculate_md5(iso_path)\n",
        "    print(f\"Original MD5: {original_md5}\")\n",
        "\n",
        "    # Initialize compressor\n",
        "    compressor = HierarchicalCompressor(\n",
        "        master_seed=args.seed,\n",
        "        num_stages=args.stages,\n",
        "        chunk_size=args.chunk_size\n",
        "    )\n",
        "\n",
        "    # Read DVD data\n",
        "    print(f\"Reading DVD data from {iso_path}...\")\n",
        "    with open(iso_path, 'rb') as f:\n",
        "        dvd_data = f.read()\n",
        "\n",
        "    # Process DVD\n",
        "    transformed_chunks, all_decisions, original_size = compressor.process_dvd(dvd_data)\n",
        "\n",
        "    # Save transformed data\n",
        "    base_name = os.path.splitext(os.path.basename(iso_path))[0]\n",
        "    transformed_file = os.path.join(args.output_dir, f\"{base_name}_transformed.bin\")\n",
        "    print(f\"Saving transformed data to {transformed_file}...\")\n",
        "    with open(transformed_file, 'wb') as f:\n",
        "        for chunk in transformed_chunks:\n",
        "            f.write(chunk)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_file = os.path.join(args.output_dir, f\"{base_name}_metadata.json\")\n",
        "    compressor.save_reconstruction_metadata(all_decisions, original_size, metadata_file)\n",
        "\n",
        "    print(\"Encoding process completed!\")\n",
        "    print(f\"- Original file: {iso_path}\")\n",
        "    print(f\"- Transformed file: {transformed_file}\")\n",
        "    print(f\"- Metadata file: {metadata_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "NO_Mk85zFGkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KfAeVhHFG1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile decoder.py\n",
        "#!/usr/bin/env python3\n",
        "# --- DVD Hierarchical Decoder ---\n",
        "# This script restores the original DVD ISO from the compressed metadata\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import hashlib\n",
        "import hmac\n",
        "import struct\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "import time\n",
        "import json\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "# Block size constants\n",
        "CHUNK_SIZE = 64 * 1024  # 64KB chunks\n",
        "\n",
        "class HierarchicalDecoder:\n",
        "    \"\"\"\n",
        "    Class to manage the hierarchical decoding process for DVD data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, master_seed, num_stages, chunk_size, num_workers=None):\n",
        "        self.master_seed = master_seed if isinstance(master_seed, bytes) else bytes.fromhex(master_seed)\n",
        "        self.num_stages = num_stages\n",
        "        self.chunk_size = chunk_size\n",
        "        self.num_workers = num_workers or (os.cpu_count() or 4)\n",
        "        self.debug_mode = False\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"Log debug messages if debug mode is enabled.\"\"\"\n",
        "        if self.debug_mode:\n",
        "            print(f\"DEBUG: {message}\")\n",
        "\n",
        "    # --- Core PRNG and Transformation Functions ---\n",
        "\n",
        "    def derive_stage_seed(self, chunk_index, stage_index):\n",
        "        \"\"\"Derive a stage-specific seed.\"\"\"\n",
        "        data = f\"{chunk_index}_{stage_index}\".encode()\n",
        "        h = hmac.new(self.master_seed, data, hashlib.sha256)\n",
        "        return h.digest()\n",
        "\n",
        "    def generate_mask_block(self, stage_seed, start_counter, block_size):\n",
        "        \"\"\"Generate a portion of the mask for XOR transformation.\"\"\"\n",
        "        digest_size = 32  # SHA-256 produces 32 bytes\n",
        "        num_digests = (block_size + digest_size - 1) // digest_size\n",
        "\n",
        "        # Pre-allocate buffer\n",
        "        mask = bytearray(num_digests * digest_size)\n",
        "        offset = 0\n",
        "\n",
        "        # Generate the mask block\n",
        "        for i in range(num_digests):\n",
        "            counter = start_counter + i\n",
        "            digest = hmac.new(stage_seed, struct.pack('>I', counter),\n",
        "                             hashlib.sha256).digest()\n",
        "            mask[offset:offset + len(digest)] = digest\n",
        "            offset += len(digest)\n",
        "\n",
        "        # Convert to NumPy array and trim\n",
        "        return np.frombuffer(mask[:block_size], dtype=np.uint8)\n",
        "\n",
        "    def apply_xor_transformation(self, data, mask):\n",
        "        \"\"\"Apply XOR transformation to data using the given mask.\"\"\"\n",
        "        return np.bitwise_xor(data, mask)\n",
        "\n",
        "    # --- Chunk Processing ---\n",
        "\n",
        "    def reverse_process_chunk(self, chunk_index, transformed_chunk, recorded_decisions):\n",
        "        \"\"\"Reverse process a single chunk using the recorded decisions.\"\"\"\n",
        "        try:\n",
        "            # Convert input to NumPy array for efficient processing\n",
        "            chunk_data = np.frombuffer(transformed_chunk, dtype=np.uint8).copy()\n",
        "\n",
        "            # Reverse the stages (last stage undone first)\n",
        "            for stage in reversed(range(self.num_stages)):\n",
        "                # Get the recorded decision for this stage\n",
        "                decision = recorded_decisions[stage]\n",
        "\n",
        "                # Skip transformation if decision was 0\n",
        "                if decision == 0:\n",
        "                    continue\n",
        "\n",
        "                # Get the stage seed\n",
        "                stage_seed = self.derive_stage_seed(chunk_index, stage)\n",
        "\n",
        "                # Process the chunk in blocks to save memory\n",
        "                for offset in range(0, len(chunk_data), self.chunk_size):\n",
        "                    # Adjust block size for the last block\n",
        "                    current_block_size = min(self.chunk_size, len(chunk_data) - offset)\n",
        "\n",
        "                    # Generate mask block (same as in forward process)\n",
        "                    counter_start = offset // 32\n",
        "                    mask_block = self.generate_mask_block(stage_seed, counter_start,\n",
        "                                                        current_block_size)\n",
        "\n",
        "                    # Apply transformation to this block (XOR is its own inverse)\n",
        "                    chunk_data[offset:offset + current_block_size] = self.apply_xor_transformation(\n",
        "                        chunk_data[offset:offset + current_block_size],\n",
        "                        mask_block\n",
        "                    )\n",
        "\n",
        "            # Convert back to bytes\n",
        "            return chunk_data.tobytes()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in reverse_process_chunk: {str(e)}\")\n",
        "            # Return transformed data on error\n",
        "            return transformed_chunk\n",
        "\n",
        "    # --- Parallel Processing ---\n",
        "\n",
        "    def reverse_process_multiple_chunks(self, transformed_chunks, all_decisions):\n",
        "        \"\"\"Reverse process multiple chunks in parallel.\"\"\"\n",
        "        recovered_chunks = [None] * len(transformed_chunks)\n",
        "\n",
        "        # Define the worker function\n",
        "        def reverse_process_chunk_worker(args):\n",
        "            chunk_idx, chunk_data, decisions = args\n",
        "            recovered = self.reverse_process_chunk(chunk_idx, chunk_data, decisions)\n",
        "            return chunk_idx, recovered\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
        "            # Submit tasks\n",
        "            futures = [executor.submit(reverse_process_chunk_worker,\n",
        "                                    (i, chunk, decisions))\n",
        "                      for i, (chunk, decisions) in enumerate(zip(transformed_chunks, all_decisions))]\n",
        "\n",
        "            total = len(futures)\n",
        "            completed = 0\n",
        "            last_percentage = 0\n",
        "\n",
        "            # Collect results with periodic progress updates\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    chunk_idx, recovered = future.result()\n",
        "                    recovered_chunks[chunk_idx] = recovered\n",
        "\n",
        "                    # Update progress at 10% intervals\n",
        "                    completed += 1\n",
        "                    percentage = int((completed / total) * 100)\n",
        "                    if percentage >= last_percentage + 10:\n",
        "                        print(f\"Recovering: {percentage}% complete ({completed}/{total} chunks)\")\n",
        "                        last_percentage = percentage\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in reverse worker thread: {str(e)}\")\n",
        "\n",
        "        # Check for unprocessed chunks\n",
        "        for i in range(len(transformed_chunks)):\n",
        "            if recovered_chunks[i] is None:\n",
        "                print(f\"Warning: Chunk {i} was not recovered, using transformed\")\n",
        "                recovered_chunks[i] = transformed_chunks[i]\n",
        "\n",
        "        return recovered_chunks\n",
        "\n",
        "    # --- DVD Processing ---\n",
        "\n",
        "    def reassemble_chunks_into_dvd(self, chunks, original_size):\n",
        "        \"\"\"Reassemble chunks back into the complete DVD data.\"\"\"\n",
        "        # Concatenate all chunks\n",
        "        data = b''.join(chunks)\n",
        "        # Trim to original size\n",
        "        return data[:original_size]\n",
        "\n",
        "    def reverse_process_dvd(self, transformed_chunks, all_decisions, original_size):\n",
        "        \"\"\"Reverse process an entire DVD.\"\"\"\n",
        "        print(f\"Recovering DVD of original size {original_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "        # Recover all chunks in parallel\n",
        "        start_time = time.time()\n",
        "        recovered_chunks = self.reverse_process_multiple_chunks(transformed_chunks, all_decisions)\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"Recovery completed in {process_time:.2f} seconds\")\n",
        "\n",
        "        # Reassemble into complete DVD\n",
        "        recovered_dvd = self.reassemble_chunks_into_dvd(recovered_chunks, original_size)\n",
        "\n",
        "        return recovered_dvd\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def load_metadata(metadata_file):\n",
        "    \"\"\"Load metadata from a JSON file.\"\"\"\n",
        "    print(f\"Loading metadata from {metadata_file}...\")\n",
        "    with open(metadata_file, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    return metadata\n",
        "\n",
        "def read_chunks(file_path, chunk_size):\n",
        "    \"\"\"Read a file in chunks of specified size.\"\"\"\n",
        "    print(f\"Reading transformed data from {file_path}...\")\n",
        "\n",
        "    # Get file size for progress reporting\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    bytes_read = 0\n",
        "    last_percentage = 0\n",
        "\n",
        "    chunks = []\n",
        "    with open(file_path, 'rb') as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            chunks.append(chunk)\n",
        "\n",
        "            # Update progress at 10% intervals\n",
        "            bytes_read += len(chunk)\n",
        "            percentage = int((bytes_read / file_size) * 100)\n",
        "            if percentage >= last_percentage + 10:\n",
        "                print(f\"Reading: {percentage}% complete ({bytes_read/(1024*1024):.1f}MB/{file_size/(1024*1024):.1f}MB)\")\n",
        "                last_percentage = percentage\n",
        "\n",
        "    print(f\"Read {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "    print(f\"Calculating MD5 of {file_path}...\")\n",
        "    md5_hash = hashlib.md5()\n",
        "\n",
        "    # Get file size for progress reporting\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    bytes_processed = 0\n",
        "    last_percentage = 0\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        # Read in chunks to handle large files\n",
        "        for chunk in iter(lambda: f.read(64 * 1024), b\"\"):\n",
        "            md5_hash.update(chunk)\n",
        "\n",
        "            # Update progress at 10% intervals\n",
        "            bytes_processed += len(chunk)\n",
        "            percentage = int((bytes_processed / file_size) * 100)\n",
        "            if percentage >= last_percentage + 10:\n",
        "                print(f\"MD5 calculation: {percentage}% complete\")\n",
        "                last_percentage = percentage\n",
        "\n",
        "    result = md5_hash.hexdigest()\n",
        "    print(f\"MD5: {result}\")\n",
        "    return result\n",
        "\n",
        "# --- Main Function ---\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"DVD Hierarchical Decoder\")\n",
        "\n",
        "    parser.add_argument(\"--transformed\", required=True,\n",
        "                        help=\"Path to transformed DVD file\")\n",
        "    parser.add_argument(\"--metadata\", required=True,\n",
        "                        help=\"Path to metadata JSON file\")\n",
        "    parser.add_argument(\"--output\", required=True,\n",
        "                        help=\"Path to save restored DVD\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create output directory if needed\n",
        "    output_dir = os.path.dirname(args.output)\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata = load_metadata(args.metadata)\n",
        "\n",
        "    # Extract parameters from metadata\n",
        "    master_seed = metadata['master_seed']\n",
        "    num_stages = metadata['num_stages']\n",
        "    chunk_size = metadata['chunk_size']\n",
        "    original_size = metadata['original_size']\n",
        "    all_decisions = metadata['decisions']\n",
        "\n",
        "    print(f\"Metadata loaded: {len(all_decisions)} chunks, {num_stages} stages per chunk\")\n",
        "\n",
        "    # Initialize decoder\n",
        "    decoder = HierarchicalDecoder(\n",
        "        master_seed=master_seed,\n",
        "        num_stages=num_stages,\n",
        "        chunk_size=chunk_size\n",
        "    )\n",
        "\n",
        "    # Read transformed chunks\n",
        "    transformed_chunks = read_chunks(args.transformed, chunk_size)\n",
        "\n",
        "    # Verify number of chunks matches metadata\n",
        "    if len(transformed_chunks) != len(all_decisions):\n",
        "        print(f\"Warning: Number of chunks ({len(transformed_chunks)}) does not match \" +\n",
        "             f\"metadata ({len(all_decisions)})\")\n",
        "\n",
        "    # Recover DVD\n",
        "    recovered_dvd = decoder.reverse_process_dvd(transformed_chunks, all_decisions, original_size)\n",
        "\n",
        "    # Save recovered DVD\n",
        "    print(f\"Saving restored DVD to {args.output}...\")\n",
        "    with open(args.output, 'wb') as f:\n",
        "        f.write(recovered_dvd)\n",
        "\n",
        "    # Calculate MD5 of restored file\n",
        "    restored_md5 = calculate_md5(args.output)\n",
        "    print(f\"Restored MD5: {restored_md5}\")\n",
        "\n",
        "    print(\"Decoding process completed!\")\n",
        "    print(f\"- Transformed file: {args.transformed}\")\n",
        "    print(f\"- Metadata file: {args.metadata}\")\n",
        "    print(f\"- Restored file: {args.output} ({os.path.getsize(args.output) / (1024*1024):.2f} MB)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FRJpQ2glvtkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBlDeWzEvt1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile validator.py\n",
        "#!/usr/bin/env python3\n",
        "# --- DVD Verification Script ---\n",
        "# This script compares the original and restored DVD files\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import hashlib\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    \"\"\"\n",
        "    Calculate MD5 hash of a file with progress updates at 10% intervals.\n",
        "    \"\"\"\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    md5_hash = hashlib.md5()\n",
        "\n",
        "    # Size of chunks to read\n",
        "    chunk_size = 1024 * 1024  # 1MB chunks\n",
        "\n",
        "    bytes_read = 0\n",
        "    last_percentage = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Calculating MD5 for {os.path.basename(file_path)} ({file_size/(1024*1024):.2f} MB)\")\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            md5_hash.update(chunk)\n",
        "            bytes_read += len(chunk)\n",
        "\n",
        "            # Update progress at 10% intervals\n",
        "            percentage = int((bytes_read / file_size) * 100)\n",
        "            if percentage >= last_percentage + 10:\n",
        "                print(f\"MD5 calculation: {percentage}% complete ({bytes_read/(1024*1024):.1f} MB processed)\")\n",
        "                last_percentage = percentage\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"MD5 calculation completed in {elapsed:.1f} seconds\")\n",
        "\n",
        "    return md5_hash.hexdigest()\n",
        "\n",
        "def compare_files(file1, file2):\n",
        "    \"\"\"\n",
        "    Compare two files byte by byte to find differences.\n",
        "    Returns: (match_status, first_diff_position, total_diffs)\n",
        "    \"\"\"\n",
        "    file1_size = os.path.getsize(file1)\n",
        "    file2_size = os.path.getsize(file2)\n",
        "\n",
        "    if file1_size != file2_size:\n",
        "        print(f\"File sizes differ: {file1_size} vs {file2_size} bytes\")\n",
        "        min_size = min("
      ],
      "metadata": {
        "id": "rmZaQR0iFsJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cc3J5plKwE0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiA2ndVvwHY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UCjDeLpwHtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DVD Hierarchical Compression Runner for Google Colab\n",
        "# This script runs the encoder, decoder, and validator in sequence\n",
        "#old\n",
        "# Install required dependencies\n",
        "!pip install numpy requests tqdm -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p output\n",
        "!mkdir -p models\n",
        "\n",
        "# Function to run a Python script and capture its output\n",
        "def run_script(script_name, args):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"RUNNING: {script_name} {' '.join(args)}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # Execute the script\n",
        "    start_time = time.time()\n",
        "    process = subprocess.Popen(\n",
        "        [sys.executable, script_name] + args,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True\n",
        "    )\n",
        "\n",
        "    # Stream the output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nCompleted in {elapsed:.2f} seconds with exit code {process.returncode}\")\n",
        "\n",
        "    return process.returncode == 0\n",
        "\n",
        "# Function to print section header\n",
        "def print_header(text):\n",
        "    print(f\"\\n\\n{'#'*60}\")\n",
        "    print(f\"# {text}\")\n",
        "    print(f\"{'#'*60}\\n\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print_header(\"DVD HIERARCHICAL COMPRESSION SYSTEM - COLAB RUNNER\")\n",
        "\n",
        "    # Ask what source to use\n",
        "    print(\"Select a source for the DVD ISO:\")\n",
        "    print(\"1. Download Alpine Linux ISO (smaller, ~130MB)\")\n",
        "    print(\"2. Download Ubuntu ISO (larger, ~3.5GB)\")\n",
        "    print(\"3. Upload a file\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-3): \").strip()\n",
        "\n",
        "    # Set parameters based on choice\n",
        "    if choice == '1':\n",
        "        print_header(\"DOWNLOADING ALPINE LINUX ISO\")\n",
        "        source_arg = \"--alpine\"\n",
        "        iso_name = \"alpine\"\n",
        "    elif choice == '2':\n",
        "        print_header(\"DOWNLOADING UBUNTU ISO\")\n",
        "        source_arg = \"--ubuntu\"\n",
        "        iso_name = \"ubuntu\"\n",
        "    elif choice == '3':\n",
        "        print_header(\"UPLOADING FILE\")\n",
        "        print(\"Please upload a file...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"No file was uploaded. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Get the uploaded file name\n",
        "        upload_filename = list(uploaded.keys())[0]\n",
        "        iso_name = os.path.splitext(upload_filename)[0]\n",
        "\n",
        "        # Move the file to the output directory\n",
        "        os.rename(upload_filename, f\"output/{upload_filename}\")\n",
        "        source_arg = f\"--file output/{upload_filename}\"\n",
        "    else:\n",
        "        print(\"Invalid choice. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Optional seed\n",
        "    use_seed = input(\"\\nUse a custom seed for reproducibility? (y/n): \").strip().lower()\n",
        "    seed_arg = \"\"\n",
        "    if use_seed == 'y':\n",
        "        seed = input(\"Enter a seed phrase: \").strip()\n",
        "        seed_arg = f\"--seed {seed}\"\n",
        "\n",
        "    # Optional block size\n",
        "    custom_block = input(\"\\nUse a custom block size? (y/n, default is 64KB): \").strip().lower()\n",
        "    block_arg = \"\"\n",
        "    if custom_block == 'y':\n",
        "        block_size = input(\"Enter block size in KB (16, 32, 64, 128, etc.): \").strip()\n",
        "        try:\n",
        "            block_bytes = int(block_size) * 1024\n",
        "            block_arg = f\"--chunk-size {block_bytes}\"\n",
        "        except ValueError:\n",
        "            print(\"Invalid block size. Using default.\")\n",
        "\n",
        "    # Construct paths\n",
        "    transformed_path = f\"output/{iso_name}_transformed.bin\"\n",
        "    metadata_path = f\"output/{iso_name}_metadata.json\"\n",
        "    restored_path = f\"output/{iso_name}_restored.iso\"\n",
        "    original_path = f\"output/{iso_name}.iso\" if choice in ['1', '2'] else f\"output/{upload_filename}\"\n",
        "\n",
        "    # Step 1: Run the encoder\n",
        "    print_header(\"STEP 1: RUNNING ENCODER\")\n",
        "\n",
        "    encoder_args = source_arg.split() + [\"--output-dir\", \"output\"]\n",
        "    if seed_arg:\n",
        "        encoder_args += seed_arg.split()\n",
        "    if block_arg:\n",
        "        encoder_args += block_arg.split()\n",
        "\n",
        "    if not run_script(\"encoder.py\", encoder_args):\n",
        "        print(\"Encoder failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Run the decoder\n",
        "    print_header(\"STEP 2: RUNNING DECODER\")\n",
        "\n",
        "    decoder_args = [\n",
        "        \"--transformed\", transformed_path,\n",
        "        \"--metadata\", metadata_path,\n",
        "        \"--output\", restored_path\n",
        "    ]\n",
        "\n",
        "    if not run_script(\"decoder.py\", decoder_args):\n",
        "        print(\"Decoder failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Run the validator\n",
        "    print_header(\"STEP 3: RUNNING VALIDATOR\")\n",
        "\n",
        "    validator_args = [\n",
        "        \"--original\", original_path,\n",
        "        \"--restored\", restored_path\n",
        "    ]\n",
        "\n",
        "    run_script(\"validator.py\", validator_args)\n",
        "\n",
        "    # Completion\n",
        "    print_header(\"PROCESS COMPLETE\")\n",
        "    print(\"Files created:\")\n",
        "    print(f\"1. Transformed data: {transformed_path}\")\n",
        "    print(f\"2. Metadata: {metadata_path}\")\n",
        "    print(f\"3. Restored ISO: {restored_path}\")\n",
        "\n",
        "    # Option to download restored file\n",
        "    download = input(\"\\nDownload the restored ISO? (y/n): \").strip().lower()\n",
        "    if download == 'y':\n",
        "        print(\"Starting download...\")\n",
        "        files.download(restored_path)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "kRCqnlFbFuJo",
        "outputId": "efb50dbe-d672-4f6c-bd9a-7eba1ea541eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-66896670f9ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-66896670f9ae>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Option to download restored file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDownload the restored ISO? (y/n): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting download...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlFaqPA-Xhur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmczx_toXiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQhIuQ23Funy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}